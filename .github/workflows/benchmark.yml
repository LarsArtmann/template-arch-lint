# ‚ö° Performance Benchmarking Pipeline
# Comprehensive benchmarking with regression detection and baseline comparison
# Tracks performance metrics across commits and detects performance regressions

name: ‚ö° Performance Benchmarks

on:
  push:
    branches: [master, main, develop]
  pull_request:
    branches: [master, main, develop]
  schedule:
    # Run benchmarks daily at 2 AM UTC to track performance trends
    - cron: "0 2 * * *"
  workflow_dispatch:
    inputs:
      benchmark_type:
        description: "Type of benchmark to run"
        required: false
        default: "all"
        type: choice
        options:
          - all
          - cpu
          - memory
          - concurrent
          - stress

# Cancel in-progress runs on new pushes to same branch
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  # Benchmark configuration
  BENCHMARK_TIME: "10s"
  BENCHMARK_COUNT: "3"
  REGRESSION_THRESHOLD: "10" # % threshold for performance regression detection
  GO_VERSION: "1.24"

jobs:
  # ==============================================
  # Baseline Benchmark Setup
  # ==============================================
  setup-baseline:
    name: üìä Setup Benchmark Baseline
    runs-on: ["self-hosted", "linux", "x64"]
    timeout-minutes: 15
    outputs:
      baseline-ref: ${{ steps.baseline.outputs.ref }}
      baseline-available: ${{ steps.check-baseline.outputs.available }}

    steps:
      - name: üì• Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Fetch full history for baseline comparison

      - name: üîç Find Baseline Reference
        id: baseline
        run: |
          # For PRs, use the target branch as baseline
          if [ "${{ github.event_name }}" == "pull_request" ]; then
            echo "ref=${{ github.event.pull_request.base.sha }}" >> $GITHUB_OUTPUT
          else
            # For pushes, use the previous commit
            echo "ref=HEAD~1" >> $GITHUB_OUTPUT
          fi

      - name: üîç Check Baseline Availability
        id: check-baseline
        run: |
          # Check if we can access the baseline commit
          if git rev-parse --verify ${{ steps.baseline.outputs.ref }} >/dev/null 2>&1; then
            echo "available=true" >> $GITHUB_OUTPUT
          else
            echo "available=false" >> $GITHUB_OUTPUT
          fi

  # ==============================================
  # CPU Performance Benchmarks
  # ==============================================
  cpu-benchmarks:
    name: üñ•Ô∏è CPU Performance Benchmarks
    runs-on: ["self-hosted", "linux", "x64"]
    timeout-minutes: 20
    if: ${{ inputs.benchmark_type == 'all' || inputs.benchmark_type == 'cpu' || inputs.benchmark_type == '' }}

    steps:
      - name: üì• Checkout Code
        uses: actions/checkout@v4

      - name: üèóÔ∏è Setup Go ${{ env.GO_VERSION }}
        uses: actions/setup-go@v5
        with:
          go-version: ${{ env.GO_VERSION }}
          cache: true
          cache-dependency-path: go.sum

      - name: üìã Download Dependencies
        run: |
          go mod download
          go mod verify

      - name: üîß Install Just and Tools
        uses: extractions/setup-just@v2
        with:
          just-version: "1.36.0"

      - name: üîß Install Required Tools
        run: |
          go install github.com/a-h/templ/cmd/templ@latest

      - name: üèóÔ∏è Build Project
        run: |
          echo "üî® Building project for benchmarks..."
          just build

      - name: üñ•Ô∏è Run CPU Benchmarks
        run: |
          echo "üñ•Ô∏è Running CPU-focused benchmarks..."
          mkdir -p benchmark-results

          # Run CPU benchmarks with multiple iterations
          just bench-cpu | tee benchmark-results/cpu-benchmarks.txt

          # Parse and format results
          echo "üìä CPU Benchmark Summary:" | tee benchmark-results/cpu-summary.txt
          grep "Benchmark" benchmark-results/cpu-benchmarks.txt | \
            awk '{print $1 ": " $3 " ns/op"}' | tee -a benchmark-results/cpu-summary.txt

      - name: üì§ Upload CPU Benchmark Results
        uses: actions/upload-artifact@v4
        with:
          name: cpu-benchmarks-${{ github.run_number }}
          path: benchmark-results/
          retention-days: 30

  # ==============================================
  # Memory Performance Benchmarks
  # ==============================================
  memory-benchmarks:
    name: üß† Memory Performance Benchmarks
    runs-on: ["self-hosted", "linux", "x64"]
    timeout-minutes: 20
    if: ${{ inputs.benchmark_type == 'all' || inputs.benchmark_type == 'memory' || inputs.benchmark_type == '' }}

    steps:
      - name: üì• Checkout Code
        uses: actions/checkout@v4

      - name: üèóÔ∏è Setup Go ${{ env.GO_VERSION }}
        uses: actions/setup-go@v5
        with:
          go-version: ${{ env.GO_VERSION }}
          cache: true
          cache-dependency-path: go.sum

      - name: üìã Download Dependencies
        run: |
          go mod download
          go mod verify

      - name: üîß Install Just and Tools
        uses: extractions/setup-just@v2
        with:
          just-version: "1.36.0"

      - name: üîß Install Required Tools
        run: |
          go install github.com/a-h/templ/cmd/templ@latest

      - name: üèóÔ∏è Build Project
        run: |
          echo "üî® Building project for benchmarks..."
          just build

      - name: üß† Run Memory Benchmarks
        run: |
          echo "üß† Running memory-focused benchmarks..."
          mkdir -p benchmark-results

          # Run memory benchmarks with allocation tracking
          just bench-memory | tee benchmark-results/memory-benchmarks.txt

          # Parse and format results
          echo "üìä Memory Benchmark Summary:" | tee benchmark-results/memory-summary.txt
          grep "Benchmark" benchmark-results/memory-benchmarks.txt | \
            awk '{print $1 ": " $3 " ns/op, " $5 " B/op, " $7 " allocs/op"}' | \
            tee -a benchmark-results/memory-summary.txt

      - name: üì§ Upload Memory Benchmark Results
        uses: actions/upload-artifact@v4
        with:
          name: memory-benchmarks-${{ github.run_number }}
          path: benchmark-results/
          retention-days: 30

  # ==============================================
  # Concurrent Performance Benchmarks
  # ==============================================
  concurrent-benchmarks:
    name: üîÑ Concurrent Performance Benchmarks
    runs-on: ["self-hosted", "linux", "x64"]
    timeout-minutes: 25
    if: ${{ inputs.benchmark_type == 'all' || inputs.benchmark_type == 'concurrent' || inputs.benchmark_type == '' }}

    steps:
      - name: üì• Checkout Code
        uses: actions/checkout@v4

      - name: üèóÔ∏è Setup Go ${{ env.GO_VERSION }}
        uses: actions/setup-go@v5
        with:
          go-version: ${{ env.GO_VERSION }}
          cache: true
          cache-dependency-path: go.sum

      - name: üìã Download Dependencies
        run: |
          go mod download
          go mod verify

      - name: üîß Install Just and Tools
        uses: extractions/setup-just@v2
        with:
          just-version: "1.36.0"

      - name: üîß Install Required Tools
        run: |
          go install github.com/a-h/templ/cmd/templ@latest

      - name: üèóÔ∏è Build Project
        run: |
          echo "üî® Building project for benchmarks..."
          just build

      - name: üîÑ Run Concurrent Benchmarks
        run: |
          echo "üîÑ Running concurrent performance benchmarks..."
          mkdir -p benchmark-results

          # Run concurrent benchmarks
          go test ./... -bench=Concurrent -benchmem -count=${{ env.BENCHMARK_COUNT }} \
            -benchtime=${{ env.BENCHMARK_TIME }} -cpu=1,2,4,8 | \
            tee benchmark-results/concurrent-benchmarks.txt

          # Parse and format results
          echo "üìä Concurrent Benchmark Summary:" | tee benchmark-results/concurrent-summary.txt
          grep "Benchmark.*Concurrent" benchmark-results/concurrent-benchmarks.txt | \
            awk '{print $1 ": " $3 " ns/op"}' | tee -a benchmark-results/concurrent-summary.txt

      - name: üì§ Upload Concurrent Benchmark Results
        uses: actions/upload-artifact@v4
        with:
          name: concurrent-benchmarks-${{ github.run_number }}
          path: benchmark-results/
          retention-days: 30

  # ==============================================
  # Stress Testing Benchmarks
  # ==============================================
  stress-benchmarks:
    name: üí™ Stress Testing Benchmarks
    runs-on: ["self-hosted", "linux", "x64"]
    timeout-minutes: 30
    if: ${{ inputs.benchmark_type == 'all' || inputs.benchmark_type == 'stress' || inputs.benchmark_type == '' }}

    steps:
      - name: üì• Checkout Code
        uses: actions/checkout@v4

      - name: üèóÔ∏è Setup Go ${{ env.GO_VERSION }}
        uses: actions/setup-go@v5
        with:
          go-version: ${{ env.GO_VERSION }}
          cache: true
          cache-dependency-path: go.sum

      - name: üìã Download Dependencies
        run: |
          go mod download
          go mod verify

      - name: üîß Install Just and Tools
        uses: extractions/setup-just@v2
        with:
          just-version: "1.36.0"

      - name: üîß Install Required Tools
        run: |
          go install github.com/a-h/templ/cmd/templ@latest

      - name: üèóÔ∏è Build Project
        run: |
          echo "üî® Building project for benchmarks..."
          just build

      - name: üí™ Run Stress Benchmarks
        run: |
          echo "üí™ Running stress testing benchmarks..."
          mkdir -p benchmark-results

          # Run stress benchmarks with extended time
          just bench-stress | tee benchmark-results/stress-benchmarks.txt

          # Parse and format results
          echo "üìä Stress Benchmark Summary:" | tee benchmark-results/stress-summary.txt
          grep "Benchmark" benchmark-results/stress-benchmarks.txt | \
            tail -20 | awk '{print $1 ": " $3 " ns/op"}' | \
            tee -a benchmark-results/stress-summary.txt

      - name: üì§ Upload Stress Benchmark Results
        uses: actions/upload-artifact@v4
        with:
          name: stress-benchmarks-${{ github.run_number }}
          path: benchmark-results/
          retention-days: 30

  # ==============================================
  # Baseline Comparison (for PRs)
  # ==============================================
  baseline-comparison:
    name: üìà Baseline Performance Comparison
    runs-on: ["self-hosted", "linux", "x64"]
    timeout-minutes: 25
    needs: [setup-baseline, cpu-benchmarks, memory-benchmarks]
    if: ${{ github.event_name == 'pull_request' && needs.setup-baseline.outputs.baseline-available == 'true' }}

    steps:
      - name: üì• Checkout Current Code
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha }}

      - name: üèóÔ∏è Setup Go ${{ env.GO_VERSION }}
        uses: actions/setup-go@v5
        with:
          go-version: ${{ env.GO_VERSION }}
          cache: true
          cache-dependency-path: go.sum

      - name: üìã Download Dependencies
        run: |
          go mod download
          go mod verify

      - name: üîß Install Just and Tools
        uses: extractions/setup-just@v2
        with:
          just-version: "1.36.0"

      - name: üîß Install Required Tools
        run: |
          go install github.com/a-h/templ/cmd/templ@latest

      - name: üèóÔ∏è Build Current Code
        run: |
          echo "üî® Building current code..."
          just build

      - name: ‚ö° Run Current Benchmarks
        run: |
          echo "‚ö° Running current benchmarks..."
          mkdir -p benchmark-results/current
          just bench-quick | tee benchmark-results/current/benchmarks.txt

      - name: üì• Checkout Baseline Code
        uses: actions/checkout@v4
        with:
          ref: ${{ needs.setup-baseline.outputs.baseline-ref }}
          path: baseline

      - name: üèóÔ∏è Build Baseline Code
        run: |
          cd baseline
          echo "üî® Building baseline code..."
          go mod download
          just build

      - name: ‚ö° Run Baseline Benchmarks
        run: |
          cd baseline
          echo "‚ö° Running baseline benchmarks..."
          mkdir -p ../benchmark-results/baseline
          just bench-quick | tee ../benchmark-results/baseline/benchmarks.txt

      - name: üìä Compare Performance
        run: |
          echo "üìä Comparing performance between baseline and current..."
          mkdir -p benchmark-results/comparison

          # Create comparison script
          cat > compare_benchmarks.go << 'EOF'
          package main

          import (
              "bufio"
              "fmt"
              "os"
              "regexp"
              "strconv"
              "strings"
          )

          type BenchResult struct {
              Name   string
              NsPerOp float64
              BytesPerOp int64
              AllocsPerOp int64
          }

          func parseBenchmarks(filename string) (map[string]BenchResult, error) {
              file, err := os.Open(filename)
              if err != nil {
                  return nil, err
              }
              defer file.Close()

              results := make(map[string]BenchResult)
              scanner := bufio.NewScanner(file)
              re := regexp.MustCompile(`^Benchmark(\w+)\s+\d+\s+([\d.]+)\s+ns/op(?:\s+([\d.]+)\s+B/op)?(?:\s+([\d.]+)\s+allocs/op)?`)

              for scanner.Scan() {
                  line := scanner.Text()
                  matches := re.FindStringSubmatch(line)
                  if len(matches) >= 3 {
                      nsPerOp, _ := strconv.ParseFloat(matches[2], 64)
                      result := BenchResult{
                          Name:    matches[1],
                          NsPerOp: nsPerOp,
                      }
                      if len(matches) > 3 && matches[3] != "" {
                          result.BytesPerOp, _ = strconv.ParseInt(matches[3], 10, 64)
                      }
                      if len(matches) > 4 && matches[4] != "" {
                          result.AllocsPerOp, _ = strconv.ParseInt(matches[4], 10, 64)
                      }
                      results[matches[1]] = result
                  }
              }
              return results, scanner.Err()
          }

          func main() {
              baseline, err := parseBenchmarks("benchmark-results/baseline/benchmarks.txt")
              if err != nil {
                  fmt.Printf("Error parsing baseline: %v\n", err)
                  return
              }

              current, err := parseBenchmarks("benchmark-results/current/benchmarks.txt")
              if err != nil {
                  fmt.Printf("Error parsing current: %v\n", err)
                  return
              }

              fmt.Println("# üìä Performance Comparison Report")
              fmt.Println()
              fmt.Println("| Benchmark | Baseline (ns/op) | Current (ns/op) | Change | Status |")
              fmt.Println("|-----------|------------------|-----------------|---------|---------|")

              regressions := 0
              improvements := 0

              for name, currentResult := range current {
                  if baselineResult, exists := baseline[name]; exists {
                      changePercent := ((currentResult.NsPerOp - baselineResult.NsPerOp) / baselineResult.NsPerOp) * 100
                      var status, change string

                      if changePercent > ${{ env.REGRESSION_THRESHOLD }} {
                          status = "‚ùå REGRESSION"
                          regressions++
                      } else if changePercent < -5 {
                          status = "‚úÖ IMPROVEMENT"
                          improvements++
                      } else {
                          status = "‚ûñ NO CHANGE"
                      }

                      if changePercent > 0 {
                          change = fmt.Sprintf("+%.1f%%", changePercent)
                      } else {
                          change = fmt.Sprintf("%.1f%%", changePercent)
                      }

                      fmt.Printf("| %s | %.1f | %.1f | %s | %s |\n",
                          name, baselineResult.NsPerOp, currentResult.NsPerOp, change, status)
                  }
              }

              fmt.Println()
              fmt.Printf("## Summary\n")
              fmt.Printf("- üî∫ Regressions: %d\n", regressions)
              fmt.Printf("- üîª Improvements: %d\n", improvements)
              fmt.Printf("- üìä Total benchmarks compared: %d\n", len(current))

              if regressions > 0 {
                  fmt.Printf("\n‚ùå Performance regressions detected! Review the changes.\n")
                  os.Exit(1)
              } else {
                  fmt.Printf("\n‚úÖ No significant performance regressions detected.\n")
              }
          }
          EOF

          # Run comparison
          go run compare_benchmarks.go | tee benchmark-results/comparison/report.md

      - name: üì§ Upload Comparison Results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-comparison-${{ github.run_number }}
          path: benchmark-results/
          retention-days: 30

      - name: üí¨ Comment PR with Results
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            try {
              const report = fs.readFileSync('benchmark-results/comparison/report.md', 'utf8');

              await github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: `## ‚ö° Performance Benchmark Results\n\n${report}\n\n---\n*Automated benchmark comparison against baseline commit ${context.payload.pull_request.base.sha.slice(0, 7)}*`
              });
            } catch (error) {
              console.log('Could not post benchmark results:', error);
            }

  # ==============================================
  # Historical Performance Tracking
  # ==============================================
  performance-tracking:
    name: üìà Performance Tracking
    runs-on: ["self-hosted", "linux", "x64"]
    timeout-minutes: 10
    if: ${{ github.event_name == 'schedule' || github.event_name == 'push' && github.ref == 'refs/heads/master' }}
    needs: [cpu-benchmarks, memory-benchmarks]

    steps:
      - name: üì• Checkout Code
        uses: actions/checkout@v4

      - name: üìä Archive Performance Metrics
        run: |
          echo "üìä Archiving performance metrics for historical tracking..."
          mkdir -p performance-history

          # Create performance record
          cat > performance-history/perf-$(date +%Y-%m-%d-%H-%M).json << EOF
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "commit": "${{ github.sha }}",
            "branch": "${{ github.ref_name }}",
            "go_version": "${{ env.GO_VERSION }}",
            "event": "${{ github.event_name }}",
            "run_number": "${{ github.run_number }}"
          }
          EOF

          echo "üìà Performance metrics archived for commit ${{ github.sha }}"

  # ==============================================
  # Benchmark Summary
  # ==============================================
  benchmark-success:
    name: ‚úÖ Benchmarks Complete
    runs-on: ["self-hosted", "linux", "x64"]
    needs: [cpu-benchmarks, memory-benchmarks, concurrent-benchmarks, stress-benchmarks]
    timeout-minutes: 2
    if: always()

    steps:
      - name: üéâ Benchmark Results Summary
        run: |
          echo "üéâ Performance benchmarking completed!"
          echo ""
          echo "üìä Benchmark Jobs Status:"
          echo "  üñ•Ô∏è  CPU Benchmarks: ${{ needs.cpu-benchmarks.result }}"
          echo "  üß† Memory Benchmarks: ${{ needs.memory-benchmarks.result }}"
          echo "  üîÑ Concurrent Benchmarks: ${{ needs.concurrent-benchmarks.result }}"
          echo "  üí™ Stress Benchmarks: ${{ needs.stress-benchmarks.result }}"
          echo ""

          # Check if any benchmark failed
          if [[ "${{ needs.cpu-benchmarks.result }}" == "failure" ]] || \
             [[ "${{ needs.memory-benchmarks.result }}" == "failure" ]] || \
             [[ "${{ needs.concurrent-benchmarks.result }}" == "failure" ]] || \
             [[ "${{ needs.stress-benchmarks.result }}" == "failure" ]]; then
            echo "‚ùå Some benchmarks failed!"
            echo "üìã Check the individual job logs for details."
            exit 1
          else
            echo "‚úÖ All benchmarks completed successfully!"
            echo "üìà Performance metrics are ready for analysis."
          fi
